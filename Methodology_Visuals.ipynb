{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d914afe2-5284-4ff2-854a-54eac8f6baa2",
   "metadata": {},
   "source": [
    "# Methodology Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23c50d-1ba8-4508-a291-e6c64d4cafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f13bc5-1638-4129-b9b5-44d17e7afc9c",
   "metadata": {},
   "source": [
    "### Densenet-121 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd522bb-5e00-4f16-87a1-5725f7ce9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "output_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaDensenetFull\"\n",
    "trial_logs_dir = os.path.join(output_dir, \"trial_logs\")\n",
    "\n",
    "\n",
    "# Load all metrics.csv logs from Optuna trials\n",
    "all_trials = []\n",
    "for trial_name in sorted(os.listdir(trial_logs_dir)):\n",
    "    trial_path = os.path.join(trial_logs_dir, trial_name, \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(trial_path):\n",
    "        df_trial = pd.read_csv(trial_path)\n",
    "        trial_id = trial_name.replace(\"trial_\", \"\")\n",
    "        df_trial[\"trial\"] = trial_id\n",
    "        all_trials.append(df_trial)\n",
    "\n",
    "if not all_trials:\n",
    "    print(\"No metrics.csv files found in:\", trial_logs_dir)\n",
    "    exit()\n",
    "\n",
    "df_all = pd.concat(all_trials, ignore_index=True)\n",
    "\n",
    "# Identify the best trial based on final validation F1 score\n",
    "if \"val_f1\" not in df_all.columns:\n",
    "    print(\"The 'val_f1' column was not found in the metric logs.\")\n",
    "    exit()\n",
    "\n",
    "last_val_f1_per_trial = (\n",
    "    df_all.dropna(subset=[\"val_f1\"])\n",
    "    .sort_values(by=[\"trial\", \"epoch\"])\n",
    "    .groupby(\"trial\")\n",
    "    .tail(1)\n",
    ")\n",
    "\n",
    "sorted_trials = last_val_f1_per_trial.sort_values(\"val_f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display best trial based on final validation F1 score\n",
    "best_trial_row = sorted_trials.iloc[0]\n",
    "best_trial_id = best_trial_row[\"trial\"]\n",
    "print(\"\\nBest Trial Based on Final Validation F1 Score:\")\n",
    "print(f\"Trial ID: {best_trial_id} | Final Val F1: {best_trial_row['val_f1']:.4f}\\n\")\n",
    "\n",
    "# Display summary of all trials sorted by final F1 score\n",
    "print(\"Summary of All Trials (Sorted by Final Val F1):\")\n",
    "for idx, row in sorted_trials.iterrows():\n",
    "    print(f\"Trial {row['trial']} | Val F1: {row['val_f1']:.4f}\")\n",
    "\n",
    "\n",
    "# Define improved plotting function for metrics\n",
    "def plot_metric(df_trial, metric_col, title):\n",
    "    if df_trial is None or metric_col not in df_trial.columns:\n",
    "        print(f\"Metric '{metric_col}' not found.\")\n",
    "        return\n",
    "\n",
    "    df_plot = df_trial.sort_values(by=\"epoch\").dropna(subset=[metric_col])\n",
    "    if df_plot.empty:\n",
    "        print(f\"No data available to plot for '{metric_col}'.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        df_plot[\"epoch\"],\n",
    "        df_plot[metric_col],\n",
    "        color=\"#2E8B57\",  # SeaGreen for main line\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2.5,\n",
    "        alpha=0.9,\n",
    "        label=f\"{metric_col.replace('_', ' ').capitalize()}\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{title}\", fontsize=18, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(metric_col.replace(\"_\", \" \").capitalize(), fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(fontsize=12, loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot validation loss and validation F1 score for the best trial\n",
    "df_best = df_all[df_all[\"trial\"] == best_trial_id]\n",
    "\n",
    "metrics_to_plot = {\n",
    "    \"val_loss\": \"Validation Loss Over Epochs\",\n",
    "    \"val_f1\": \"Validation F1 Score Over Epochs\"\n",
    "}\n",
    "\n",
    "for metric_col, title in metrics_to_plot.items():\n",
    "    plot_metric(df_best, metric_col, title)\n",
    "\n",
    "# Load your study\n",
    "study = joblib.load(r\"C:\\Users\\Xuxu\\Desktop\\Master Thesis\\OptunaDensenetFull\\new_densenet_study.pkl\")\n",
    "\n",
    "# Plot optimization history\n",
    "fig1 = vis.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# Plot hyperparameter importance\n",
    "fig2 = vis.plot_param_importances(study)\n",
    "fig2.show()\n",
    "\n",
    "# Plot parallel coordinates\n",
    "fig3 = vis.plot_parallel_coordinate(study)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f0ba2-e7e1-4e98-b69f-55e4605d01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get DataFrame\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f6752-d680-4482-b574-50bbe58c9f5e",
   "metadata": {},
   "source": [
    "### ConvNext-Tiny Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f28ec-5364-4796-937a-27d131d662b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaConvNeXtFull\"\n",
    "trial_logs_dir = os.path.join(output_dir, \"trial_logs\")\n",
    "\n",
    "# load all metrics.csv logs from optuna trials\n",
    "all_trials = []\n",
    "for trial_name in sorted(os.listdir(trial_logs_dir)):\n",
    "    trial_path = os.path.join(trial_logs_dir, trial_name, \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(trial_path):\n",
    "        df_trial = pd.read_csv(trial_path)\n",
    "        trial_id = trial_name.replace(\"trial_\", \"\")\n",
    "        df_trial[\"trial\"] = trial_id\n",
    "        all_trials.append(df_trial)\n",
    "\n",
    "if not all_trials:\n",
    "    print(\"no metrics.csv files found in:\", trial_logs_dir)\n",
    "    exit()\n",
    "\n",
    "df_all = pd.concat(all_trials, ignore_index=True)\n",
    "\n",
    "# identify the best trial based on final validation f1 score\n",
    "if \"val_f1\" not in df_all.columns:\n",
    "    print(\"the 'val_f1' column was not found in the metric logs.\")\n",
    "    exit()\n",
    "\n",
    "last_val_f1_per_trial = (\n",
    "    df_all.dropna(subset=[\"val_f1\"])\n",
    "    .sort_values(by=[\"trial\", \"epoch\"])\n",
    "    .groupby(\"trial\")\n",
    "    .tail(1)\n",
    ")\n",
    "\n",
    "sorted_trials = last_val_f1_per_trial.sort_values(\"val_f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# display best trial based on final validation f1 score\n",
    "best_trial_row = sorted_trials.iloc[0]\n",
    "best_trial_id = best_trial_row[\"trial\"]\n",
    "print(\"\\nbest trial based on final validation f1 score:\")\n",
    "print(f\"trial id: {best_trial_id} | final val f1: {best_trial_row['val_f1']:.4f}\\n\")\n",
    "\n",
    "# display summary of all trials sorted by final f1 score\n",
    "print(\"summary of all trials (sorted by final val f1):\")\n",
    "for idx, row in sorted_trials.iterrows():\n",
    "    print(f\"trial {row['trial']} | val f1: {row['val_f1']:.4f}\")\n",
    "\n",
    "# load optuna study and best hyperparameters\n",
    "pkl_path = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaConvNeXtFull/new_convnext_study.pkl\"\n",
    "study = joblib.load(pkl_path)\n",
    "\n",
    "print(\"\\nbest hyperparameters (from optuna):\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# define improved plotting function for metrics\n",
    "def plot_metric(df_trial, metric_col, title):\n",
    "    if df_trial is None or metric_col not in df_trial.columns:\n",
    "        print(f\"metric '{metric_col}' not found.\")\n",
    "        return\n",
    "\n",
    "    df_plot = df_trial.sort_values(by=\"epoch\").dropna(subset=[metric_col])\n",
    "    if df_plot.empty:\n",
    "        print(f\"no data available to plot for '{metric_col}'.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        df_plot[\"epoch\"],\n",
    "        df_plot[metric_col],\n",
    "        color=\"#2E8B57\",  # seagreen for main line\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2.5,\n",
    "        alpha=0.9,\n",
    "        label=f\"{metric_col.replace('_', ' ').capitalize()}\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{title}\", fontsize=18, fontweight=\"bold\")\n",
    "    plt.xlabel(\"epoch\", fontsize=14)\n",
    "    plt.ylabel(metric_col.replace(\"_\", \" \").capitalize(), fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(fontsize=12, loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot validation loss and validation f1 score for the best trial\n",
    "df_best = df_all[df_all[\"trial\"] == best_trial_id]\n",
    "\n",
    "metrics_to_plot = {\n",
    "    \"val_loss\": \"validation loss over epochs\",\n",
    "    \"val_f1\": \"validation f1 score over epochs\"\n",
    "}\n",
    "\n",
    "for metric_col, title in metrics_to_plot.items():\n",
    "    plot_metric(df_best, metric_col, title)\n",
    "\n",
    "# load your study\n",
    "study = joblib.load(r\"C:\\Users\\Xuxu\\Desktop\\Master Thesis\\OptunaConvNeXtFull\\new_convnext_study.pkl\")\n",
    "\n",
    "# plot optimization history\n",
    "fig1 = vis.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# plot hyperparameter importance\n",
    "fig2 = vis.plot_param_importances(study)\n",
    "fig2.show()\n",
    "\n",
    "# plot parallel coordinates\n",
    "fig3 = vis.plot_parallel_coordinate(study)\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eee0cf-8e01-453f-bcbb-99792aa2e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get DataFrame\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f221417-aa77-4484-83b5-879edae42968",
   "metadata": {},
   "source": [
    "### EfficientNet-B0 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbbe1e-4208-40d1-b499-555a61b6e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaEfficientNetB0Full\"\n",
    "trial_logs_dir = os.path.join(output_dir, \"trial_logs\")\n",
    "\n",
    "# load all metrics.csv logs from optuna trials\n",
    "all_trials = []\n",
    "for trial_name in sorted(os.listdir(trial_logs_dir)):\n",
    "    trial_path = os.path.join(trial_logs_dir, trial_name, \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(trial_path):\n",
    "        df_trial = pd.read_csv(trial_path)\n",
    "        trial_id = trial_name.replace(\"trial_\", \"\")\n",
    "        df_trial[\"trial\"] = trial_id\n",
    "        all_trials.append(df_trial)\n",
    "\n",
    "if not all_trials:\n",
    "    print(\"no metrics.csv files found in:\", trial_logs_dir)\n",
    "    exit()\n",
    "\n",
    "df_all = pd.concat(all_trials, ignore_index=True)\n",
    "\n",
    "# identify the best trial based on final validation f1 score\n",
    "if \"val_f1\" not in df_all.columns:\n",
    "    print(\"the 'val_f1' column was not found in the metric logs.\")\n",
    "    exit()\n",
    "\n",
    "last_val_f1_per_trial = (\n",
    "    df_all.dropna(subset=[\"val_f1\"])\n",
    "    .sort_values(by=[\"trial\", \"epoch\"])\n",
    "    .groupby(\"trial\")\n",
    "    .tail(1)\n",
    ")\n",
    "\n",
    "sorted_trials = last_val_f1_per_trial.sort_values(\"val_f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# display best trial based on final validation f1 score\n",
    "best_trial_row = sorted_trials.iloc[0]\n",
    "best_trial_id = best_trial_row[\"trial\"]\n",
    "print(\"\\nbest trial based on final validation f1 score:\")\n",
    "print(f\"trial id: {best_trial_id} | final val f1: {best_trial_row['val_f1']:.4f}\\n\")\n",
    "\n",
    "# display summary of all trials sorted by final f1 score\n",
    "print(\"summary of all trials (sorted by final val f1):\")\n",
    "for idx, row in sorted_trials.iterrows():\n",
    "    print(f\"trial {row['trial']} | val f1: {row['val_f1']:.4f}\")\n",
    "\n",
    "# load optuna study and best hyperparameters\n",
    "pkl_path = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaEfficientNetB0Full/new_efficientnet_study.pkl\"\n",
    "study = joblib.load(pkl_path)\n",
    "\n",
    "print(\"\\nbest hyperparameters (from optuna):\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# define improved plotting function for metrics\n",
    "def plot_metric(df_trial, metric_col, title):\n",
    "    if df_trial is None or metric_col not in df_trial.columns:\n",
    "        print(f\"metric '{metric_col}' not found.\")\n",
    "        return\n",
    "\n",
    "    df_plot = df_trial.sort_values(by=\"epoch\").dropna(subset=[metric_col])\n",
    "    if df_plot.empty:\n",
    "        print(f\"no data available to plot for '{metric_col}'.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        df_plot[\"epoch\"],\n",
    "        df_plot[metric_col],\n",
    "        color=\"#2E8B57\",  # seagreen for main line\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2.5,\n",
    "        alpha=0.9,\n",
    "        label=f\"{metric_col.replace('_', ' ').capitalize()}\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{title}\", fontsize=18, fontweight=\"bold\")\n",
    "    plt.xlabel(\"epoch\", fontsize=14)\n",
    "    plt.ylabel(metric_col.replace(\"_\", \" \").capitalize(), fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(fontsize=12, loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot validation loss and validation f1 score for the best trial\n",
    "df_best = df_all[df_all[\"trial\"] == best_trial_id]\n",
    "\n",
    "metrics_to_plot = {\n",
    "    \"val_loss\": \"validation loss over epochs\",\n",
    "    \"val_f1\": \"validation f1 score over epochs\"\n",
    "}\n",
    "\n",
    "for metric_col, title in metrics_to_plot.items():\n",
    "    plot_metric(df_best, metric_col, title)\n",
    "\n",
    "# load your study\n",
    "study = joblib.load(r\"C:\\Users\\Xuxu\\Desktop\\Master Thesis\\OptunaEfficientNetB0Full\\new_efficientnet_study.pkl\")\n",
    "\n",
    "# plot optimization history\n",
    "fig1 = vis.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# plot hyperparameter importance\n",
    "fig2 = vis.plot_param_importances(study)\n",
    "fig2.show()\n",
    "\n",
    "# plot parallel coordinates\n",
    "fig3 = vis.plot_parallel_coordinate(study)\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a43bf8-e11c-477d-8163-9975e673c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get DataFrame\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01803025-91b9-497d-a8e9-296eb731731b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e33a6d-f244-4e55-b175-62174d0103fe",
   "metadata": {},
   "source": [
    "### Densenet-121 + Best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d7f34-5846-43c0-8a2b-271f9b614e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "save_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/BestHyperDensenet121Full\"\n",
    "class_idx_path = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaDensenetFull/class_to_idx.json\"\n",
    "num_folds = 5\n",
    "\n",
    "# load class mapping\n",
    "with open(class_idx_path, \"r\") as f:\n",
    "    class_to_idx = json.load(f)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "# load predictions across folds\n",
    "all_preds_all_folds = []\n",
    "all_targets_all_folds = []\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    fold_dir = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\")\n",
    "    preds_path = os.path.join(fold_dir, \"all_preds.npy\")\n",
    "    targets_path = os.path.join(fold_dir, \"all_targets.npy\")\n",
    "\n",
    "    if os.path.exists(preds_path) and os.path.exists(targets_path):\n",
    "        all_preds_all_folds.extend(np.load(preds_path))\n",
    "        all_targets_all_folds.extend(np.load(targets_path))\n",
    "\n",
    "# convert predictions and targets to numpy arrays\n",
    "all_preds_all_folds = np.array(all_preds_all_folds)\n",
    "all_targets_all_folds = np.array(all_targets_all_folds)\n",
    "\n",
    "# fold-wise final metrics summary\n",
    "metrics_cols = [\n",
    "    \"train_f1\", \"train_precision\", \"train_recall\", \"train_loss\",\n",
    "    \"val_f1\", \"val_precision\", \"val_recall\", \"val_loss\"\n",
    "]\n",
    "metrics_summary = []\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    metrics_file = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file)\n",
    "\n",
    "        df_val = df.dropna(subset=[\"val_f1\"])\n",
    "        df_train = df.dropna(subset=[\"train_f1\"])\n",
    "\n",
    "        last_val = df_val.sort_values(\"epoch\").groupby(\"epoch\").tail(1).iloc[-1]\n",
    "        last_train = df_train.sort_values(\"epoch\").groupby(\"epoch\").tail(1).iloc[-1]\n",
    "\n",
    "        row = [\n",
    "            last_train.get(\"train_f1\", np.nan),\n",
    "            last_train.get(\"train_precision\", np.nan),\n",
    "            last_train.get(\"train_recall\", np.nan),\n",
    "            last_train.get(\"train_loss\", np.nan),\n",
    "            last_val.get(\"val_f1\", np.nan),\n",
    "            last_val.get(\"val_precision\", np.nan),\n",
    "            last_val.get(\"val_recall\", np.nan),\n",
    "            last_val.get(\"val_loss\", np.nan),\n",
    "        ]\n",
    "        metrics_summary.append(row)\n",
    "\n",
    "# convert to dataframe\n",
    "df_summary = pd.DataFrame(metrics_summary, columns=metrics_cols)\n",
    "df_summary.index = [f\"fold {i}\" for i in range(num_folds)]\n",
    "\n",
    "# display fold-wise metrics summary\n",
    "print(\"\\n=== fold-wise metrics summary ===\")\n",
    "for idx, row in df_summary.iterrows():\n",
    "    print(f\"{idx}: \"\n",
    "          f\"train f1: {row['train_f1']:.4f}, \"\n",
    "          f\"val f1: {row['val_f1']:.4f}, \"\n",
    "          f\"val precision: {row['val_precision']:.4f}, \"\n",
    "          f\"val recall: {row['val_recall']:.4f}, \"\n",
    "          f\"val loss: {row['val_loss']:.4f}\")\n",
    "\n",
    "# display mean and standard deviation across folds\n",
    "print(\"\\n=== mean and standard deviation across folds ===\")\n",
    "mean_std = df_summary.agg([\"mean\", \"std\"]).round(4)\n",
    "\n",
    "print(f\"mean train f1: {mean_std.loc['mean', 'train_f1']:.4f}, std: {mean_std.loc['std', 'train_f1']:.4f}\")\n",
    "print(f\"mean val f1:   {mean_std.loc['mean', 'val_f1']:.4f}, std: {mean_std.loc['std', 'val_f1']:.4f}\")\n",
    "print(f\"mean val precision: {mean_std.loc['mean', 'val_precision']:.4f}, std: {mean_std.loc['std', 'val_precision']:.4f}\")\n",
    "print(f\"mean val recall:    {mean_std.loc['mean', 'val_recall']:.4f}, std: {mean_std.loc['std', 'val_recall']:.4f}\")\n",
    "print(f\"mean val loss:      {mean_std.loc['mean', 'val_loss']:.4f}, std: {mean_std.loc['std', 'val_loss']:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf920eb-b7c0-4b89-89a8-836a508140b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify best fold based on final validation f1\n",
    "best_fold = None\n",
    "best_val_f1 = -1\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    path = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df_val = df[df[\"val_f1\"].notna()].copy().reset_index(drop=True)\n",
    "        if not df_val.empty:\n",
    "            final_val_f1 = df_val.iloc[-1][\"val_f1\"]\n",
    "            if final_val_f1 > best_val_f1:\n",
    "                best_val_f1 = final_val_f1\n",
    "                best_fold = fold\n",
    "\n",
    "# plot training and validation f1 for the best fold\n",
    "if best_fold is not None:\n",
    "    path = os.path.join(save_dir, f\"fold_{best_fold}\", \"version_0\", \"metrics.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df_train = df[df[\"train_f1\"].notna()].copy().reset_index(drop=True)\n",
    "    df_val = df[df[\"val_f1\"].notna()].copy().reset_index(drop=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df_train[\"epoch\"], df_train[\"train_f1\"], label=\"train f1\", color=\"#2E8B57\", linewidth=2.5)\n",
    "    plt.plot(df_val[\"epoch\"], df_val[\"val_f1\"], label=\"validation f1\", color=\"#B22222\", linewidth=2.5)\n",
    "\n",
    "    plt.xlabel(\"epoch\", fontsize=14)\n",
    "    plt.ylabel(\"f1 score\", fontsize=14)\n",
    "    plt.title(f\"training and validation f1 score (best fold {best_fold})\", fontsize=18, fontweight=\"bold\")\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # updated legend position\n",
    "    plt.legend(fontsize=12, loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6a8b9-34e9-4ce0-8751-421ce2a10bec",
   "metadata": {},
   "source": [
    "### EfficientNet-B0 + Best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8846b-33f2-4c0f-9f72-2ae8e55e5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "save_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/BestHyperEfficientNetB0Full\"\n",
    "class_idx_path = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaDensenetFull/class_to_idx.json\"\n",
    "num_folds = 5\n",
    "\n",
    "# load class mapping\n",
    "with open(class_idx_path, \"r\") as f:\n",
    "    class_to_idx = json.load(f)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "# load predictions across folds\n",
    "all_preds_all_folds = []\n",
    "all_targets_all_folds = []\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    fold_dir = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\")\n",
    "    preds_path = os.path.join(fold_dir, \"all_preds.npy\")\n",
    "    targets_path = os.path.join(fold_dir, \"all_targets.npy\")\n",
    "\n",
    "    if os.path.exists(preds_path) and os.path.exists(targets_path):\n",
    "        all_preds_all_folds.extend(np.load(preds_path))\n",
    "        all_targets_all_folds.extend(np.load(targets_path))\n",
    "\n",
    "# convert predictions and targets to numpy arrays\n",
    "all_preds_all_folds = np.array(all_preds_all_folds)\n",
    "all_targets_all_folds = np.array(all_targets_all_folds)\n",
    "\n",
    "# fold-wise final metrics summary\n",
    "metrics_cols = [\n",
    "    \"train_f1\", \"train_precision\", \"train_recall\", \"train_loss\",\n",
    "    \"val_f1\", \"val_precision\", \"val_recall\", \"val_loss\"\n",
    "]\n",
    "metrics_summary = []\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    metrics_file = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file)\n",
    "\n",
    "        df_val = df.dropna(subset=[\"val_f1\"])\n",
    "        df_train = df.dropna(subset=[\"train_f1\"])\n",
    "\n",
    "        last_val = df_val.sort_values(\"epoch\").groupby(\"epoch\").tail(1).iloc[-1]\n",
    "        last_train = df_train.sort_values(\"epoch\").groupby(\"epoch\").tail(1).iloc[-1]\n",
    "\n",
    "        row = [\n",
    "            last_train.get(\"train_f1\", np.nan),\n",
    "            last_train.get(\"train_precision\", np.nan),\n",
    "            last_train.get(\"train_recall\", np.nan),\n",
    "            last_train.get(\"train_loss\", np.nan),\n",
    "            last_val.get(\"val_f1\", np.nan),\n",
    "            last_val.get(\"val_precision\", np.nan),\n",
    "            last_val.get(\"val_recall\", np.nan),\n",
    "            last_val.get(\"val_loss\", np.nan),\n",
    "        ]\n",
    "        metrics_summary.append(row)\n",
    "\n",
    "# convert to dataframe\n",
    "df_summary = pd.DataFrame(metrics_summary, columns=metrics_cols)\n",
    "df_summary.index = [f\"fold {i}\" for i in range(num_folds)]\n",
    "\n",
    "# display fold-wise metrics summary\n",
    "print(\"\\nmetrics summary per fold\")\n",
    "for idx, row in df_summary.iterrows():\n",
    "    print(f\"{idx}: \"\n",
    "          f\"train f1: {row['train_f1']:.4f}, \"\n",
    "          f\"val f1: {row['val_f1']:.4f}, \"\n",
    "          f\"val precision: {row['val_precision']:.4f}, \"\n",
    "          f\"val recall: {row['val_recall']:.4f}, \"\n",
    "          f\"val loss: {row['val_loss']:.4f}\")\n",
    "\n",
    "# display mean and standard deviation across folds\n",
    "print(\"\\nmean and standard deviation across folds\")\n",
    "mean_std = df_summary.agg([\"mean\", \"std\"]).round(4)\n",
    "\n",
    "print(f\"mean train f1: {mean_std.loc['mean', 'train_f1']:.4f}, std: {mean_std.loc['std', 'train_f1']:.4f}\")\n",
    "print(f\"mean val f1:   {mean_std.loc['mean', 'val_f1']:.4f}, std: {mean_std.loc['std', 'val_f1']:.4f}\")\n",
    "print(f\"mean val precision: {mean_std.loc['mean', 'val_precision']:.4f}, std: {mean_std.loc['std', 'val_precision']:.4f}\")\n",
    "print(f\"mean val recall:    {mean_std.loc['mean', 'val_recall']:.4f}, std: {mean_std.loc['std', 'val_recall']:.4f}\")\n",
    "print(f\"mean val loss:      {mean_std.loc['mean', 'val_loss']:.4f}, std: {mean_std.loc['std', 'val_loss']:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d94e6-7829-4c9f-a3b9-f5f5b03cb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify best fold based on final validation f1\n",
    "best_fold = None\n",
    "best_val_f1 = -1\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    path = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df_val = df[df[\"val_f1\"].notna()].copy().reset_index(drop=True)\n",
    "        if not df_val.empty:\n",
    "            final_val_f1 = df_val.iloc[-1][\"val_f1\"]\n",
    "            if final_val_f1 > best_val_f1:\n",
    "                best_val_f1 = final_val_f1\n",
    "                best_fold = fold\n",
    "\n",
    "# plot training and validation f1 for the best fold\n",
    "if best_fold is not None:\n",
    "    path = os.path.join(save_dir, f\"fold_{best_fold}\", \"version_0\", \"metrics.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df_train = df[df[\"train_f1\"].notna()].copy().reset_index(drop=True)\n",
    "    df_val = df[df[\"val_f1\"].notna()].copy().reset_index(drop=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df_train[\"epoch\"], df_train[\"train_f1\"], label=\"train f1\", color=\"#2E8B57\", linewidth=2.5)\n",
    "    plt.plot(df_val[\"epoch\"], df_val[\"val_f1\"], label=\"validation f1\", color=\"#B22222\", linewidth=2.5)\n",
    "\n",
    "    plt.xlabel(\"epoch\", fontsize=14)\n",
    "    plt.ylabel(\"f1 score\", fontsize=14)\n",
    "    plt.title(f\"training and validation f1 score (best fold {best_fold})\", fontsize=18, fontweight=\"bold\")\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # updated legend position\n",
    "    plt.legend(fontsize=12, loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53eab3f-87e7-4a27-b518-8571b78ecca3",
   "metadata": {},
   "source": [
    "### ConvNeXtTiny + Best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06209a76-537b-44e9-af06-577247447e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# configuration\n",
    "save_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/BestHyperConvNeXtFull\"\n",
    "class_idx_path = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaDensenetFull/class_to_idx.json\"\n",
    "num_folds = 5\n",
    "\n",
    "# load class mapping\n",
    "with open(class_idx_path, \"r\") as f:\n",
    "    class_to_idx = json.load(f)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "# load predictions across folds\n",
    "all_preds_all_folds = []\n",
    "all_targets_all_folds = []\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    fold_dir = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\")\n",
    "    preds_path = os.path.join(fold_dir, \"all_preds.npy\")\n",
    "    targets_path = os.path.join(fold_dir, \"all_targets.npy\")\n",
    "\n",
    "    if os.path.exists(preds_path) and os.path.exists(targets_path):\n",
    "        all_preds_all_folds.extend(np.load(preds_path))\n",
    "        all_targets_all_folds.extend(np.load(targets_path))\n",
    "\n",
    "# convert predictions and targets to numpy arrays\n",
    "all_preds_all_folds = np.array(all_preds_all_folds)\n",
    "all_targets_all_folds = np.array(all_targets_all_folds)\n",
    "\n",
    "# fold-wise final metrics summary\n",
    "metrics_cols = [\n",
    "    \"train_f1\", \"train_precision\", \"train_recall\", \"train_loss\",\n",
    "    \"val_f1\", \"val_precision\", \"val_recall\", \"val_loss\"\n",
    "]\n",
    "metrics_summary = []\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    metrics_file = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file)\n",
    "\n",
    "        df_val = df.dropna(subset=[\"val_f1\"])\n",
    "        df_train = df.dropna(subset=[\"train_f1\"])\n",
    "\n",
    "        last_val = df_val.sort_values(\"epoch\").groupby(\"epoch\").tail(1).iloc[-1]\n",
    "        last_train = df_train.sort_values(\"epoch\").groupby(\"epoch\").tail(1).iloc[-1]\n",
    "\n",
    "        row = [\n",
    "            last_train.get(\"train_f1\", np.nan),\n",
    "            last_train.get(\"train_precision\", np.nan),\n",
    "            last_train.get(\"train_recall\", np.nan),\n",
    "            last_train.get(\"train_loss\", np.nan),\n",
    "            last_val.get(\"val_f1\", np.nan),\n",
    "            last_val.get(\"val_precision\", np.nan),\n",
    "            last_val.get(\"val_recall\", np.nan),\n",
    "            last_val.get(\"val_loss\", np.nan),\n",
    "        ]\n",
    "        metrics_summary.append(row)\n",
    "\n",
    "# convert to dataframe\n",
    "df_summary = pd.DataFrame(metrics_summary, columns=metrics_cols)\n",
    "df_summary.index = [f\"fold {i}\" for i in range(num_folds)]\n",
    "\n",
    "# display fold-wise metrics summary\n",
    "print(\"\\nmetrics summary per fold\")\n",
    "for idx, row in df_summary.iterrows():\n",
    "    print(f\"{idx}: \"\n",
    "          f\"train f1: {row['train_f1']:.4f}, \"\n",
    "          f\"val f1: {row['val_f1']:.4f}, \"\n",
    "          f\"val precision: {row['val_precision']:.4f}, \"\n",
    "          f\"val recall: {row['val_recall']:.4f}, \"\n",
    "          f\"val loss: {row['val_loss']:.4f}\")\n",
    "\n",
    "# display mean and standard deviation across folds\n",
    "print(\"\\nmean and standard deviation across folds\")\n",
    "mean_std = df_summary.agg([\"mean\", \"std\"]).round(4)\n",
    "\n",
    "print(f\"mean train f1: {mean_std.loc['mean', 'train_f1']:.4f}, std: {mean_std.loc['std', 'train_f1']:.4f}\")\n",
    "print(f\"mean val f1:   {mean_std.loc['mean', 'val_f1']:.4f}, std: {mean_std.loc['std', 'val_f1']:.4f}\")\n",
    "print(f\"mean val precision: {mean_std.loc['mean', 'val_precision']:.4f}, std: {mean_std.loc['std', 'val_precision']:.4f}\")\n",
    "print(f\"mean val recall:    {mean_std.loc['mean', 'val_recall']:.4f}, std: {mean_std.loc['std', 'val_recall']:.4f}\")\n",
    "print(f\"mean val loss:      {mean_std.loc['mean', 'val_loss']:.4f}, std: {mean_std.loc['std', 'val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2e400-750b-4bd9-bfa5-06a54b59dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify best fold based on final validation f1\n",
    "best_fold = None\n",
    "best_val_f1 = -1\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    path = os.path.join(save_dir, f\"fold_{fold}\", \"version_0\", \"metrics.csv\")\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df_val = df[df[\"val_f1\"].notna()].copy().reset_index(drop=True)\n",
    "        if not df_val.empty:\n",
    "            final_val_f1 = df_val.iloc[-1][\"val_f1\"]\n",
    "            if final_val_f1 > best_val_f1:\n",
    "                best_val_f1 = final_val_f1\n",
    "                best_fold = fold\n",
    "\n",
    "# plot training and validation f1 for the best fold\n",
    "if best_fold is not None:\n",
    "    path = os.path.join(save_dir, f\"fold_{best_fold}\", \"version_0\", \"metrics.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df_train = df[df[\"train_f1\"].notna()].copy().reset_index(drop=True)\n",
    "    df_val = df[df[\"val_f1\"].notna()].copy().reset_index(drop=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df_train[\"epoch\"], df_train[\"train_f1\"], label=\"train f1\", color=\"#2E8B57\", linewidth=2.5)\n",
    "    plt.plot(df_val[\"epoch\"], df_val[\"val_f1\"], label=\"validation f1\", color=\"#B22222\", linewidth=2.5)\n",
    "\n",
    "    plt.xlabel(\"epoch\", fontsize=14)\n",
    "    plt.ylabel(\"f1 score\", fontsize=14)\n",
    "    plt.title(f\"training and validation f1 score (best fold {best_fold})\", fontsize=18, fontweight=\"bold\")\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # updated legend position\n",
    "    plt.legend(fontsize=12, loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ba88c-5a19-4799-a4ba-28c274acbc9c",
   "metadata": {},
   "source": [
    "### ConvNext-Tiny(Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c1fe0-df74-4f24-aa0e-94c66d12116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# paths\n",
    "version_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/SupervisedBaselineVer2Epoch100/single_run/version_0\"\n",
    "index_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaDensenetFull\"\n",
    "save_dir = os.path.join(version_dir, \"figures\")\n",
    "\n",
    "# create save_dir if needed\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# file paths\n",
    "preds_path = os.path.join(version_dir, \"all_preds.npy\")\n",
    "targets_path = os.path.join(version_dir, \"all_targets.npy\")\n",
    "probs_path = os.path.join(version_dir, \"all_probs.npy\")\n",
    "metrics_path = os.path.join(version_dir, \"test_metrics.json\")\n",
    "\n",
    "# load class mapping\n",
    "with open(os.path.join(index_dir, \"class_to_idx.json\")) as f:\n",
    "    class_to_idx = json.load(f)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "# load predictions and targets\n",
    "all_preds = np.load(preds_path)\n",
    "all_targets = np.load(targets_path)\n",
    "all_probs = np.load(probs_path)\n",
    "\n",
    "# classification report\n",
    "print(\"\\n# classification report\")\n",
    "print(classification_report(all_targets, all_preds, target_names=class_names, digits=4))\n",
    "\n",
    "# confusion matrix \n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Greens\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    annot_kws={\"fontsize\": 6}\n",
    ")\n",
    "plt.title(\"convnext-tiny (supervised) – confusion matrix\", fontsize=14)\n",
    "plt.xlabel(\"predicted label\", fontsize=12)\n",
    "plt.ylabel(\"true label\", fontsize=12)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.savefig(os.path.join(save_dir, \"confusion_matrix_absolute_cleaned.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# roc curves (multi-class)\n",
    "n_classes = len(class_names)\n",
    "y_true_bin = label_binarize(all_targets, classes=list(range(n_classes)))\n",
    "\n",
    "# compute roc and auc\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# micro-average\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), all_probs.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# macro-average\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# save auc values\n",
    "roc_auc_json = {class_names[i]: roc_auc[i] for i in range(n_classes)}\n",
    "roc_auc_json[\"micro\"] = roc_auc[\"micro\"]\n",
    "roc_auc_json[\"macro\"] = roc_auc[\"macro\"]\n",
    "with open(os.path.join(save_dir, \"roc_auc_scores.json\"), \"w\") as f:\n",
    "    json.dump(roc_auc_json, f, indent=4)\n",
    "\n",
    "# plot roc curves\n",
    "plt.figure(figsize=(7, 6), dpi=300)\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"{class_names[i]} (auc = {roc_auc[i]:.2f})\", linewidth=1.5)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f\"micro-average (auc = {roc_auc['micro']:.2f})\", color='deeppink', linestyle=':', linewidth=2)\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], label=f\"macro-average (auc = {roc_auc['macro']:.2f})\", color='navy', linestyle='-.', linewidth=2)\n",
    "plt.title(\"convnext-tiny (supervised) – multi-class roc curve\", fontsize=14)\n",
    "plt.xlabel(\"false positive rate\", fontsize=12)\n",
    "plt.ylabel(\"true positive rate\", fontsize=12)\n",
    "plt.legend(loc=\"lower right\", fontsize=7)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.savefig(os.path.join(save_dir, \"roc_curve_multiclass.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c88f7-2b68-4128-bb34-546ff35e44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training metrics and drop any rows missing training loss or f1\n",
    "df = pd.read_csv(metrics_path)\n",
    "df_epoch = df.dropna(subset=[\"train_loss\", \"train_f1\"]).reset_index(drop=True)\n",
    "\n",
    "# load predicted and true labels, then calculate macro f1-score\n",
    "all_preds = np.load(preds_path)\n",
    "all_targets = np.load(targets_path)\n",
    "report = classification_report(all_targets, all_preds, output_dict=True, zero_division=0)\n",
    "macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "# create plot for training and test metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot training loss over epochs\n",
    "plt.plot(\n",
    "    df_epoch[\"epoch\"],\n",
    "    df_epoch[\"train_loss\"],\n",
    "    label=\"training loss\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"#8B4513\",  # saddlebrown\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# plot training f1 score over epochs\n",
    "plt.plot(\n",
    "    df_epoch[\"epoch\"],\n",
    "    df_epoch[\"train_f1\"],\n",
    "    label=\"training f1\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"#228B22\",  # forestgreen\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# plot horizontal line for final test loss if available\n",
    "if \"test_loss\" in df.columns and not df[\"test_loss\"].isnull().all():\n",
    "    test_loss = df[\"test_loss\"].dropna().values[-1]\n",
    "    plt.hlines(\n",
    "        y=test_loss,\n",
    "        xmin=df_epoch[\"epoch\"].min(),\n",
    "        xmax=df_epoch[\"epoch\"].max(),\n",
    "        label=f\"final test loss ({test_loss:.4f})\",\n",
    "        colors=\"#A0522D\",  # sienna\n",
    "        linestyles=\"--\",\n",
    "        linewidth=2.0\n",
    "    )\n",
    "\n",
    "# plot horizontal line for final test macro f1 score\n",
    "plt.axhline(\n",
    "    y=macro_f1,\n",
    "    xmin=0,\n",
    "    xmax=1,\n",
    "    label=f\"final test macro f1 ({macro_f1:.4f})\",\n",
    "    color=\"#006400\",  # darkgreen\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2.0\n",
    ")\n",
    "\n",
    "# format axes and layout\n",
    "plt.xlabel(\"epoch\", fontsize=14)\n",
    "plt.ylabel(\"metric value\", fontsize=14)\n",
    "plt.title(\"convnext-tiny (supervised) – training and test metrics\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot and display it\n",
    "save_path = os.path.join(save_dir, \"convnext_training_metrics.png\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cab78-81b2-4e3d-9cd2-a16839a54e0d",
   "metadata": {},
   "source": [
    "### ConvNext-Tiny(BYOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6e071-31a6-4250-8c0f-c2dccad63929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "version_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/BYOLBaselineVer2Epoch100/single_run/version_0\"\n",
    "index_dir = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaDensenetFull\"\n",
    "save_dir = os.path.join(version_dir, \"figures\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "preds_path = os.path.join(version_dir, \"all_preds.npy\")\n",
    "targets_path = os.path.join(version_dir, \"all_targets.npy\")\n",
    "probs_path = os.path.join(version_dir, \"all_probs.npy\")\n",
    "metrics_path = os.path.join(version_dir, \"test_metrics.json\")\n",
    "\n",
    "# load class names\n",
    "with open(os.path.join(index_dir, \"class_to_idx.json\")) as f:\n",
    "    class_to_idx = json.load(f)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "# load predictions and test metrics\n",
    "all_preds = np.load(preds_path)\n",
    "all_targets = np.load(targets_path)\n",
    "all_probs = np.load(probs_path)\n",
    "\n",
    "print(\"\\n# classification report\")\n",
    "print(classification_report(all_targets, all_preds, target_names=class_names, digits=4))\n",
    "\n",
    "# confusion matrix (absolute)\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={\"fontsize\": 6})\n",
    "plt.title(\"convnext-tiny (byol) – confusion matrix\", fontsize=14)\n",
    "plt.xlabel(\"predicted label\", fontsize=12)\n",
    "plt.ylabel(\"true label\", fontsize=12)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.savefig(os.path.join(save_dir, \"confusion_matrix_absolute_cleaned.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# roc curve setup\n",
    "n_classes = len(class_names)\n",
    "y_true_bin = label_binarize(all_targets, classes=list(range(n_classes)))\n",
    "\n",
    "# compute per-class roc and auc\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# micro-average\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), all_probs.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# macro-average\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# save auc scores\n",
    "roc_auc_json = {class_names[i]: roc_auc[i] for i in range(n_classes)}\n",
    "roc_auc_json[\"micro\"] = roc_auc[\"micro\"]\n",
    "roc_auc_json[\"macro\"] = roc_auc[\"macro\"]\n",
    "with open(os.path.join(save_dir, \"roc_auc_scores.json\"), \"w\") as f:\n",
    "    json.dump(roc_auc_json, f, indent=4)\n",
    "\n",
    "# plot roc curves\n",
    "plt.figure(figsize=(7, 6), dpi=300)\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i],\n",
    "             label=f\"{c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bc987-2062-4f44-8be6-037ab54795d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training metrics and filter rows with valid loss and f1 values\n",
    "df = pd.read_csv(metrics_path)\n",
    "df_epoch = df.dropna(subset=[\"train_loss\", \"train_f1\"]).reset_index(drop=True)\n",
    "\n",
    "# load predicted and true labels, then compute macro-averaged f1 score\n",
    "all_preds = np.load(preds_path)\n",
    "all_targets = np.load(targets_path)\n",
    "report = classification_report(all_targets, all_preds, output_dict=True, zero_division=0)\n",
    "macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "# initialize the plot for training and test metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot training loss across epochs\n",
    "plt.plot(\n",
    "    df_epoch[\"epoch\"],\n",
    "    df_epoch[\"train_loss\"],\n",
    "    label=\"training loss\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"#8B4513\",  # saddlebrown\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# plot training f1 score across epochs\n",
    "plt.plot(\n",
    "    df_epoch[\"epoch\"],\n",
    "    df_epoch[\"train_f1\"],\n",
    "    label=\"training f1\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"#228B22\",  # forestgreen\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# draw horizontal line for final test loss if available\n",
    "if \"test_loss\" in df.columns and not df[\"test_loss\"].isnull().all():\n",
    "    test_loss = df[\"test_loss\"].dropna().values[-1]\n",
    "    plt.hlines(\n",
    "        y=test_loss,\n",
    "        xmin=df_epoch[\"epoch\"].min(),\n",
    "        xmax=df_epoch[\"epoch\"].max(),\n",
    "        label=f\"final test loss ({test_loss:.4f})\",\n",
    "        colors=\"#A0522D\",  # sienna\n",
    "        linestyles=\"--\",\n",
    "        linewidth=2.0\n",
    "    )\n",
    "\n",
    "# draw horizontal line for final macro-averaged test f1 score\n",
    "plt.axhline(\n",
    "    y=macro_f1,\n",
    "    xmin=0,\n",
    "    xmax=1,\n",
    "    label=f\"final test macro f1 ({macro_f1:.4f})\",\n",
    "    color=\"#006400\",  # darkgreen\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2.0\n",
    ")\n",
    "\n",
    "# set axis labels and title\n",
    "plt.xlabel(\"epoch\", fontsize=14)\n",
    "plt.ylabel(\"metric value\", fontsize=14)\n",
    "plt.title(\"convnext-tiny (by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a5ea8-f1bf-40ba-a9b4-047f1bf19587",
   "metadata": {},
   "source": [
    "### Supervised ConvNextTiny + SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908c1cb-327a-48a0-a849-73926114ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define paths\n",
    "VERSION_DIR = r\"C:/Users/Xuxu/Desktop/Master Thesis/SIMCLRBaselineEpoch100/single_run/version_0\"\n",
    "INDEX_DIR = r\"C:/Users/Xuxu/Desktop/Master Thesis/OptunaDensenetFull\"\n",
    "SAVE_DIR = os.path.join(VERSION_DIR, \"figures\")\n",
    "\n",
    "# Create directory to save figures if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Define file paths\n",
    "PREDS_PATH = os.path.join(VERSION_DIR, \"all_preds.npy\")\n",
    "TARGETS_PATH = os.path.join(VERSION_DIR, \"all_targets.npy\")\n",
    "PROBS_PATH = os.path.join(VERSION_DIR, \"all_probs.npy\")\n",
    "METRICS_PATH = os.path.join(VERSION_DIR, \"test_metrics.json\")\n",
    "\n",
    "# Load class names\n",
    "with open(os.path.join(INDEX_DIR, \"class_to_idx.json\")) as f:\n",
    "    class_to_idx = json.load(f)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "# Load predictions, targets, and probabilities\n",
    "all_preds = np.load(PREDS_PATH)\n",
    "all_targets = np.load(TARGETS_PATH)\n",
    "all_probs = np.load(PROBS_PATH)\n",
    "\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n# Classification Report\")\n",
    "print(classification_report(all_targets, all_preds, target_names=class_names, digits=4))\n",
    "\n",
    "# Plot confusion matrix (absolute counts)\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Greens\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    annot_kws={\"fontsize\": 6}\n",
    ")\n",
    "plt.title(\"ConvNeXt-Tiny (SimCLR) – Confusion Matrix\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"confusion_matrix_absolute_cleaned.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Prepare for ROC Curve (multi-class)\n",
    "n_classes = len(class_names)\n",
    "y_true_bin = label_binarize(all_targets, classes=list(range(n_classes)))\n",
    "\n",
    "# Compute ROC curve and AUC for each class\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and AUC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), all_probs.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and AUC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(7, 6), dpi=300)\n",
    "\n",
    "# Plot ROC for each class\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i],\n",
    "             label=f\"{class_names[i]} (AUC = {roc_auc[i]:.2f})\",\n",
    "             linewidth=1.5)\n",
    "\n",
    "# Plot micro- and macro-average ROC\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f\"Micro-average (AUC = {roc_auc['micro']:.2f})\",\n",
    "         color='deeppink', linestyle=':', linewidth=2)\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label=f\"Macro-average (AUC = {roc_auc['macro']:.2f})\",\n",
    "         color='navy', linestyle='-.', linewidth=2)\n",
    "\n",
    "# Finalize ROC plot\n",
    "plt.title(\"ConvNeXt-Tiny (SimCLR) – Multi-Class ROC Curve\", fontsize=14)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.legend(loc=\"lower right\", fontsize=7)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"roc_curve_byol.png\"), bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a9899-ee01-4e8f-87a8-4ad31877fbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load training metrics and keep only rows with valid training loss and f1 score\n",
    "df = pd.read_csv(metrics_path)\n",
    "df_epoch = df.dropna(subset=[\"train_loss\", \"train_f1\"]).reset_index(drop=True)\n",
    "\n",
    "# load predicted and true labels, then compute macro-averaged f1 score\n",
    "all_preds = np.load(preds_path)\n",
    "all_targets = np.load(targets_path)\n",
    "report = classification_report(all_targets, all_preds, output_dict=True, zero_division=0)\n",
    "macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "# create the plot for training and test metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot training loss across epochs\n",
    "plt.plot(\n",
    "    df_epoch[\"epoch\"],\n",
    "    df_epoch[\"train_loss\"],\n",
    "    label=\"training loss\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"#8B4513\",  # saddlebrown\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# plot training f1 score across epochs\n",
    "plt.plot(\n",
    "    df_epoch[\"epoch\"],\n",
    "    df_epoch[\"train_f1\"],\n",
    "    label=\"training f1\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"#228B22\",  # forestgreen\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# plot horizontal line for final test loss if available\n",
    "if \"test_loss\" in df.columns and not df[\"test_loss\"].isnull().all():\n",
    "    test_loss = df[\"test_loss\"].dropna().values[-1]\n",
    "    plt.hlines(\n",
    "        y=test_loss,\n",
    "        xmin=df_epoch[\"epoch\"].min(),\n",
    "        xmax=df_epoch[\"epoch\"].max(),\n",
    "        label=f\"final test loss ({test_loss:.4f})\",\n",
    "        colors=\"#A0522D\",  # sienna\n",
    "        linestyles=\"--\",\n",
    "        linewidth=2.0\n",
    "    )\n",
    "\n",
    "# plot horizontal line for final macro-averaged test f1 score\n",
    "plt.axhline(\n",
    "    y=macro_f1,\n",
    "    xmin=0,\n",
    "    xmax=1,\n",
    "    label=f\"final test macro f1 ({macro_f1:.4f})\",\n",
    "    color=\"#006400\",  # darkgreen\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2.0\n",
    ")\n",
    "\n",
    "# set axis labels and plot title\n",
    "plt.xlabel(\"epoch\", fontsize=14)\n",
    "plt.ylabel(\"metric value\", fontsize=14)\n",
    "plt.title(\"convnext-tiny (simclr) – training and test metrics\", fontsize=16)\n",
    "\n",
    "# customize tick size and legend\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot to file and display it\n",
    "save_path = os.path.join(save_dir, \"convnext_training_metrics.png\")\n",
    "plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
